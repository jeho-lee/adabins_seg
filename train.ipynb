{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import shutil\n",
    "import logging\n",
    "import time\n",
    "import timeit\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Distributed training\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import model_io\n",
    "import models\n",
    "import utils\n",
    "from dataloader import DepthDataLoader\n",
    "# from loss import SILogLoss, BinsChamferLoss\n",
    "from utils import RunningAverage, colorize\n",
    "import matplotlib\n",
    "\n",
    "from datasets import Cityscapes\n",
    "from loss import CrossEntropy\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "def is_distributed():\n",
    "    return dist.is_initialized()\n",
    "\n",
    "def get_world_size():\n",
    "    if not dist.is_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "def get_rank():\n",
    "    if not dist.is_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "def get_sampler(dataset):\n",
    "    if is_distributed():\n",
    "        from torch.utils.data.distributed import DistributedSampler\n",
    "        return DistributedSampler(dataset)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def is_rank_zero(args):\n",
    "    return args.rank == 0\n",
    "\n",
    "def reduce_tensor(inp):\n",
    "    \"\"\"\n",
    "    Reduce the loss from all processes so that \n",
    "    process with rank 0 has the averaged results.\n",
    "    \"\"\"\n",
    "    world_size = dist.get_world_size()\n",
    "    if world_size < 2:\n",
    "        return inp\n",
    "    with torch.no_grad():\n",
    "        reduced_inp = inp\n",
    "        torch.distributed.reduce(reduced_inp, dst=0)\n",
    "    return reduced_inp / world_size\n",
    "\n",
    "######################################\n",
    "seed = 304\n",
    "gpus = [0,]\n",
    "local_rank = 0\n",
    "\n",
    "# Cityscapes\n",
    "data_root = '/newHDD/datasets/Cityscapes/'\n",
    "train_list = 'train.lst'\n",
    "val_list = 'val.lst'\n",
    "######################################\n",
    "\n",
    "import argparse\n",
    "\n",
    "# Define the argument parser (this part remains the same)\n",
    "parser = argparse.ArgumentParser(description='Training script. Default values of all arguments are recommended for reproducibility', fromfile_prefix_chars='@',\n",
    "                                 conflict_handler='resolve')\n",
    "parser.add_argument('--epochs', default=25, type=int, help='number of total epochs to run')\n",
    "parser.add_argument('--n-bins', '--n_bins', default=80, type=int, help='number of bins/buckets to divide depth range into')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.000357, type=float, help='max learning rate')\n",
    "parser.add_argument('--wd', '--weight-decay', default=0.1, type=float, help='weight decay')\n",
    "parser.add_argument('--w_chamfer', '--w-chamfer', default=0.1, type=float, help=\"weight value for chamfer loss\")\n",
    "parser.add_argument('--div-factor', '--div_factor', default=25, type=float, help=\"Initial div factor for lr\")\n",
    "parser.add_argument('--final-div-factor', '--final_div_factor', default=100, type=float, help=\"final div factor for lr\")\n",
    "parser.add_argument('--batch_size', default=4, type=int, help='batch size')\n",
    "parser.add_argument('--validate-every', '--validate_every', default=100, type=int, help='validation period')\n",
    "parser.add_argument('--gpu', default=None, type=int, help='Which gpu to use')\n",
    "\n",
    "parser.add_argument(\"--norm\", default=\"linear\", type=str, help=\"Type of norm/competition for bin-widths\", choices=['linear', 'softmax', 'sigmoid'])\n",
    "parser.add_argument(\"--same-lr\", '--same_lr', default=False, action=\"store_true\", help=\"Use same LR for all param groups\")\n",
    "parser.add_argument(\"--distributed\", default=False, action=\"store_true\", help=\"Use DDP if set\")\n",
    "\n",
    "parser.add_argument(\"--root\", default=\"./experiments\", type=str, help=\"Root folder to save data in\")\n",
    "parser.add_argument(\"--name\", default=\"UnetAdaptiveBins\")\n",
    "parser.add_argument(\"--resume\", default='', type=str, help=\"Resume from checkpoint\")\n",
    "\n",
    "parser.add_argument(\"--notes\", default='', type=str, help=\"Wandb notes\")\n",
    "parser.add_argument(\"--tags\", default='sweep', type=str, help=\"Wandb tags\")\n",
    "parser.add_argument(\"--workers\", default=11, type=int, help=\"Number of workers for data loading\")\n",
    "parser.add_argument(\"--dataset\", default='nyu', type=str, help=\"Dataset to train on\")\n",
    "parser.add_argument(\"--data_path\", default='../dataset/nyu/sync/', type=str, help=\"path to dataset\")\n",
    "parser.add_argument(\"--gt_path\", default='../dataset/nyu/sync/', type=str, help=\"path to dataset\")\n",
    "parser.add_argument('--filenames_file', default=\"./train_test_inputs/nyudepthv2_train_files_with_gt.txt\", type=str, help='path to the filenames text file')\n",
    "parser.add_argument('--input_height', type=int, help='input height', default=416)\n",
    "parser.add_argument('--input_width', type=int, help='input width', default=544)\n",
    "parser.add_argument('--max_depth', type=float, help='maximum depth in estimation', default=10)\n",
    "parser.add_argument('--min_depth', type=float, help='minimum depth in estimation', default=1e-3)\n",
    "parser.add_argument('--do_random_rotate', default=True, help='if set, will perform random rotation for augmentation', action='store_true')\n",
    "parser.add_argument('--degree', type=float, help='random rotation maximum degree', default=2.5)\n",
    "parser.add_argument('--do_kb_crop', help='if set, crop input images as kitti benchmark images', action='store_true')\n",
    "parser.add_argument('--use_right', help='if set, will randomly use right images when train on KITTI', action='store_true')\n",
    "parser.add_argument('--data_path_eval', default=\"../dataset/nyu/official_splits/test/\", type=str, help='path to the data for online evaluation')\n",
    "parser.add_argument('--gt_path_eval', default=\"../dataset/nyu/official_splits/test/\", type=str, help='path to the groundtruth data for online evaluation')\n",
    "parser.add_argument('--filenames_file_eval', default=\"./train_test_inputs/nyudepthv2_test_files_with_gt.txt\", type=str, help='path to the filenames text file for online evaluation')\n",
    "parser.add_argument('--min_depth_eval', type=float, help='minimum depth for evaluation', default=1e-3)\n",
    "parser.add_argument('--max_depth_eval', type=float, help='maximum depth for evaluation', default=10)\n",
    "parser.add_argument('--eigen_crop', default=True, help='if set, crops according to Eigen NIPS14', action='store_true')\n",
    "parser.add_argument('--garg_crop', help='if set, crops according to Garg ECCV16', action='store_true')\n",
    "\n",
    "# Segmentation\n",
    "parser.add_argument('--n_semantic_classes', help='Number of semantic classes', default=19, type=int)\n",
    "parser.add_argument('--img_width', help='Width of input image', default=1024, type=int) \n",
    "parser.add_argument('--img_height', help='Height of input image', default=512, type=int)\n",
    "parser.add_argument('--base_size', help='Base size of input image', default=2048, type=int) # Cityscapes original size: 2048x1024\n",
    "parser.add_argument('--ignore_label', help='Label to ignore', default=255, type=int)\n",
    "\n",
    "arg_list = [\n",
    "    \"--batch_size\", \"4\",\n",
    "]\n",
    "args = parser.parse_args(arg_list)\n",
    "\n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    args.gpu = gpu\n",
    "    \n",
    "    crop_size = (args.img_height, args.img_width)\n",
    "    train_dataset = Cityscapes(root=data_root,\n",
    "                            list_path=train_list,\n",
    "                            num_samples=None,\n",
    "                            num_classes=args.n_semantic_classes,\n",
    "                            multi_scale=True,\n",
    "                            flip=True,\n",
    "                            ignore_label=args.ignore_label,\n",
    "                            base_size=args.base_size,\n",
    "                            crop_size=crop_size,\n",
    "                            downsample_rate=1,\n",
    "                            scale_factor=16)\n",
    "\n",
    "    train_sampler = get_sampler(train_dataset)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            sampler=train_sampler)\n",
    "    \n",
    "    val_dataset = Cityscapes(root=data_root,\n",
    "                            list_path=val_list,\n",
    "                            num_samples=None,\n",
    "                            num_classes=args.n_semantic_classes,\n",
    "                            multi_scale=False,\n",
    "                            flip=False,\n",
    "                            ignore_label=args.ignore_label,\n",
    "                            base_size=args.base_size,\n",
    "                            crop_size=crop_size,\n",
    "                            downsample_rate=1)\n",
    "\n",
    "    val_sampler = get_sampler(val_dataset)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        sampler=val_sampler)\n",
    "\n",
    "    criterion_entropy = CrossEntropy(ignore_label=args.ignore_label,\n",
    "                            weight=train_dataset.class_weights)\n",
    "\n",
    "    print(\"Load model\")\n",
    "    model = models.UnetAdaptiveSegmentation.build(n_classes=args.n_semantic_classes)\n",
    "\n",
    "    if args.gpu is not None:  # If a gpu is set by user: NO PARALLELISM!!\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        model = model.cuda(args.gpu)\n",
    "\n",
    "    args.multigpu = False\n",
    "    if args.distributed:\n",
    "        # Use DDP\n",
    "        args.multigpu = True\n",
    "        args.rank = args.rank * ngpus_per_node + gpu\n",
    "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                world_size=args.world_size, rank=args.rank)\n",
    "        args.batch_size = int(args.batch_size / ngpus_per_node)\n",
    "        # args.batch_size = 8\n",
    "        args.workers = int((args.num_workers + ngpus_per_node - 1) / ngpus_per_node)\n",
    "        print(args.gpu, args.rank, args.batch_size, args.workers)\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        model = nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        model = model.cuda(args.gpu)\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], output_device=args.gpu,\n",
    "                                                            find_unused_parameters=True)\n",
    "    elif args.gpu is None:\n",
    "        # Use DP\n",
    "        args.multigpu = True\n",
    "        model = model.cuda()\n",
    "        model = torch.nn.DataParallel(model)\n",
    "            \n",
    "    args.epoch = 0\n",
    "    args.last_epoch = -1\n",
    "\n",
    "    # train(model, args, epochs=args.epochs, lr=args.lr, device=args.gpu, root=args.root,\n",
    "    #       experiment_name=args.name, optimizer_state_dict=None)\n",
    "\n",
    "    should_write = ((not args.distributed) or args.rank == 0)\n",
    "    \n",
    "    if args.gpu is None:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = args.gpu\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    if args.same_lr:\n",
    "        print(\"Using same LR\")\n",
    "        params = model.parameters()\n",
    "    else:\n",
    "        print(\"Using diff LR\")\n",
    "        m = model.module if args.multigpu else model\n",
    "        params = [{\"params\": m.get_1x_lr_params(), \"lr\": args.lr / 10},\n",
    "                    {\"params\": m.get_10x_lr_params(), \"lr\": args.lr}]\n",
    "\n",
    "    optimizer = optim.AdamW(params, weight_decay=args.wd, lr=args.lr)\n",
    "\n",
    "    iters = len(train_loader)\n",
    "    step = args.epoch * iters\n",
    "    # best_loss = np.inf\n",
    "    best_mIoU = 0\n",
    "\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                            args.lr, \n",
    "                                            epochs=args.epochs, \n",
    "                                            steps_per_epoch=len(train_loader),\n",
    "                                            cycle_momentum=True,\n",
    "                                            base_momentum=0.85, max_momentum=0.95, last_epoch=args.last_epoch,\n",
    "                                            div_factor=args.div_factor, final_div_factor=args.final_div_factor)\n",
    "    \n",
    "    print(\"Start training\")\n",
    "    for cur_epoch in range(args.epoch, args.epochs):\n",
    "        for i, batch in tqdm(enumerate(train_loader), desc=f\"Epoch: {cur_epoch + 1}/{args.epochs}. Loop: Train\",\n",
    "                                total=len(train_loader)) if is_rank_zero(args) else enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            images, labels, _, _ = batch\n",
    "            images = images.cuda()\n",
    "            labels = labels.long().cuda()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion_entropy(outputs, labels)\n",
    "            \n",
    "            # model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # if step % 10 == 0 and dist.get_rank() == 0:\n",
    "            if step % 10 == 0:\n",
    "                msg = 'Epoch: [{}/{}] Step:[{}], lr: {}, Loss: {:.6f}'.format(cur_epoch, args.epochs, step, [x['lr'] for x in optimizer.param_groups], loss)\n",
    "            \n",
    "            step += 1\n",
    "            scheduler.step()\n",
    "            \n",
    "            if should_write and step % args.validate_every == 0:\n",
    "                \n",
    "                model.eval()\n",
    "                valid_loss, mean_IoU, IoU_array = validate(args, val_loader, model, criterion_entropy, cur_epoch, args.epochs, device)\n",
    "\n",
    "                model_io.save_checkpoint(model, optimizer, cur_epoch, f\"{args.name}_latest.pt\",\n",
    "                                            root=os.path.join(args.root, \"checkpoints\"))\n",
    "\n",
    "                if mean_IoU > best_mIoU:\n",
    "                    model_io.save_checkpoint(model, optimizer, cur_epoch, f\"{args.name}_best.pt\",\n",
    "                                            root=os.path.join(args.root, \"checkpoints\"))\n",
    "                    best_mIoU = mean_IoU\n",
    "\n",
    "                model.train()\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.val = None\n",
    "        self.avg = None\n",
    "        self.sum = None\n",
    "        self.count = None\n",
    "\n",
    "    def initialize(self, val, weight):\n",
    "        self.val = val\n",
    "        self.avg = val\n",
    "        self.sum = val * weight\n",
    "        self.count = weight\n",
    "        self.initialized = True\n",
    "\n",
    "    def update(self, val, weight=1):\n",
    "        if not self.initialized:\n",
    "            self.initialize(val, weight)\n",
    "        else:\n",
    "            self.add(val, weight)\n",
    "\n",
    "    def add(self, val, weight):\n",
    "        self.val = val\n",
    "        self.sum += val * weight\n",
    "        self.count += weight\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def value(self):\n",
    "        return self.val\n",
    "\n",
    "    def average(self):\n",
    "        return self.avg\n",
    "    \n",
    "def get_confusion_matrix(label, pred, size, num_class, ignore=-1):\n",
    "    \"\"\"\n",
    "    Calcute the confusion matrix by given label and pred\n",
    "    \"\"\"\n",
    "    output = pred.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    seg_pred = np.asarray(np.argmax(output, axis=3), dtype=np.uint8)\n",
    "    seg_gt = np.asarray(\n",
    "    label.cpu().numpy()[:, :size[-2], :size[-1]], dtype=int)\n",
    "\n",
    "    ignore_index = seg_gt != ignore\n",
    "    seg_gt = seg_gt[ignore_index]\n",
    "    seg_pred = seg_pred[ignore_index]\n",
    "\n",
    "    index = (seg_gt * num_class + seg_pred).astype('int32')\n",
    "    label_count = np.bincount(index)\n",
    "    confusion_matrix = np.zeros((num_class, num_class))\n",
    "\n",
    "    for i_label in range(num_class):\n",
    "        for i_pred in range(num_class):\n",
    "            cur_index = i_label * num_class + i_pred\n",
    "            if cur_index < len(label_count):\n",
    "                confusion_matrix[i_label,\n",
    "                                 i_pred] = label_count[cur_index]\n",
    "    return confusion_matrix\n",
    "\n",
    "def validate(args, test_loader, model, criterion, epoch, epochs, device='cuda'):\n",
    "    \"\"\"\n",
    "    Validation function to compute loss, confusion matrix, and IoU metrics.\n",
    "    Args:\n",
    "        args: Command-line arguments including hyperparameters and paths.\n",
    "        test_loader: DataLoader for validation data.\n",
    "        model: Model to be evaluated.\n",
    "        criterion: Loss function (e.g., CrossEntropy).\n",
    "        epoch: Current epoch for logging.\n",
    "        epochs: Total number of epochs.\n",
    "        device: Device to use for computation ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    ave_loss = AverageMeter()\n",
    "    nums = args.n_semantic_classes\n",
    "    confusion_matrix = np.zeros((nums, nums))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            images, labels, _, _ = batch\n",
    "            size = labels.size()\n",
    "            images = images.to(device)\n",
    "            labels = labels.long().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Resize the output to match the target size\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                outputs = [F.interpolate(output, size=size[-2:], mode='bilinear', align_corners=True) for output in outputs]\n",
    "            else:\n",
    "                outputs = F.interpolate(outputs, size=size[-2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            reduced_loss = reduce_tensor(loss) if args.distributed else loss\n",
    "            ave_loss.update(reduced_loss.item())\n",
    "\n",
    "            # Compute confusion matrix\n",
    "            if isinstance(outputs, (list, tuple)):\n",
    "                for output in outputs:\n",
    "                    confusion_matrix += get_confusion_matrix(\n",
    "                        labels, output, size, args.n_semantic_classes, args.ignore_label\n",
    "                    )\n",
    "            else:\n",
    "                confusion_matrix += get_confusion_matrix(\n",
    "                    labels, outputs, size, args.n_semantic_classes, args.ignore_label\n",
    "                )\n",
    "\n",
    "            if idx % 10 == 0 and is_rank_zero(args):\n",
    "                print(f\"Validation: Iteration [{idx}/{len(test_loader)}], Loss: {reduced_loss.item()}\")\n",
    "\n",
    "    # Reduce confusion matrix across GPUs\n",
    "    if args.distributed:\n",
    "        confusion_matrix = torch.from_numpy(confusion_matrix).to(device)\n",
    "        reduced_confusion_matrix = reduce_tensor(confusion_matrix)\n",
    "        confusion_matrix = reduced_confusion_matrix.cpu().numpy()\n",
    "\n",
    "    # Compute mean IoU\n",
    "    pos = confusion_matrix.sum(1)\n",
    "    res = confusion_matrix.sum(0)\n",
    "    tp = np.diag(confusion_matrix)\n",
    "    IoU_array = tp / np.maximum(1.0, pos + res - tp)\n",
    "    mean_IoU = IoU_array.mean()\n",
    "\n",
    "    if is_rank_zero(args):\n",
    "        print(f\"Epoch: {epoch}/{epochs}, Validation Loss: {ave_loss.average():.6f}, Mean IoU: {mean_IoU:.6f}\")\n",
    "        logging.info(f\"Epoch: {epoch}/{epochs}, IoU per class: {IoU_array}, Mean IoU: {mean_IoU:.6f}\")\n",
    "\n",
    "    return ave_loss.average(), mean_IoU, IoU_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if seed > 0:\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "args.num_threads = args.workers\n",
    "args.mode = 'train'\n",
    "    \n",
    "# try:\n",
    "#     node_str = os.environ['SLURM_JOB_NODELIST'].replace('[', '').replace(']', '')\n",
    "#     nodes = node_str.split(',')\n",
    "#     args.world_size = len(nodes)\n",
    "#     args.rank = int(os.environ['SLURM_PROCID'])\n",
    "# except KeyError as e:\n",
    "#     # We are NOT using SLURM\n",
    "#     args.world_size = 1\n",
    "#     args.rank = 0\n",
    "#     nodes = [\"127.0.0.1\"]\n",
    "args.world_size = 1\n",
    "args.rank = 0\n",
    "nodes = [\"127.0.0.1\"]\n",
    "\n",
    "# if args.distributed:\n",
    "#     mp.set_start_method('forkserver')\n",
    "#     print(args.rank)\n",
    "#     port = np.random.randint(15000, 15025)\n",
    "#     args.dist_url = 'tcp://{}:{}'.format(nodes[0], port)\n",
    "#     print(args.dist_url)\n",
    "#     args.dist_backend = 'nccl'\n",
    "#     args.gpu = None\n",
    "\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "args.num_workers = args.workers\n",
    "args.ngpus_per_node = ngpus_per_node\n",
    "\n",
    "if args.distributed:\n",
    "    args.world_size = ngpus_per_node * args.world_size\n",
    "    mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
    "else:\n",
    "    if ngpus_per_node == 1:\n",
    "        args.gpu = 0\n",
    "    main_worker(args.gpu, ngpus_per_node, args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
